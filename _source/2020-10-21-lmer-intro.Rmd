---
title: "Intro to Mixed Effects Regression in R"
author: allie
categories: [ tutorial ]
image: assets/images/2020-10-21-lmer-intro/dragon.png
featured: true
hidden: false
output:
  html_document: default
  pdf_document: default
  md_document:
    variant: gfm
    preserve_yaml: TRUE
---

```{r setup, include=FALSE, cache=FALSE}
knitr::opts_knit$set(base.dir="../", base.url="/")
knitr::opts_chunk$set(fig.path="assets/images/2020-10-21-lmer-intro/",
                      cache.path = '../cache/',
                      message=FALSE, warning=FALSE,
                      cache=TRUE, echo=TRUE)
```

<br><br><br>

Welcome! This is an intro-level workshop about mixed effects regression in R. We'll cover the basics of linear and logit models. You should have an intermediate-level understanding of R and standard linear regression. 

<br><br>

Acknowledgments: Adapted from code provided by Gabriela K Hajduk (gkhajduk.github.io), who in turn referenced a workshop developed by Liam Bailey. Parts of the tutorial are also adapted from a lesson on partial pooling by Tristan Mahr. 

For further reading, please check out their tutorials and blogs here:
<br>
https://gkhajduk.github.io/2017-03-09-mixed-models/
<br>
https://www.tjmahr.com/plotting-partial-pooling-in-mixed-effects-models/


<br><br><br>

## Setup

<br>
First, we'll just get everything set up. We need to tweak some settings, load packages, and read our data.

```{r echo=T, results='hide', error=FALSE, warning=FALSE, message=FALSE}
#change some settings
options(scipen=999) #turn off scientific notation
options(contrasts = c("contr.sum","contr.poly")) #this tweaks the sum-of-squares settings to make sure the output of Anova(model) and summary(model) are consistent and appropriate when a model has interaction terms

#time to load some packages!
library(lme4) #fit the models
library(lmerTest) #gives p-values and more info
library(car) #more settings for regression output
library(dplyr) #for data wrangling
library(tibble) #for data wrangling
library(sjPlot) #plotting model-predicted values
library(ggplot2) #plotting raw data
library(data.table) #for pretty HTML tables of model parameters

#load the data
dragons <- read.csv("2020-10-21-dragon-data.csv")
```


<br><br><br><br><br>


## Data
<br>
Let's get familiar with our dataset. This is a fictional dataset about dragons. Each dragon has one row. We have information about each dragon's body length and cognitive test score. Let's say our first research question is whether the length of the dragon is related to its intelligence. 

We also have some other information about each dragon: We know about the mountain range where it lives, color, diet, and whether or not it breathes fire.

<br><br><br>

Take a look at the data and check the counts of our variables.

```{r Explore}
#take a peek at the header
head(dragons)

#view the full dataset
#View(dragons)

#check out counts for all our categorical variables
table(dragons$mountainRange)
table(dragons$diet)
table(dragons$color)
table(dragons$breathesFire)
```

<br><br><br>

Let's check distributions. Do test scores and body length measurements look approximately normal?

```{r Distributions}
#check assumptions: do our continuous variables have approximately normal distributions?
hist(dragons$testScore)

hist(dragons$bodyLength)
```

<br><br><br>

We should mean-center our continuous measure of body length before using it in a model.

```{r Standardization}
#It is good practice to  standardise your explanatory variables before proceeding - you can use scale() to do that:
dragons$bodyLength_s <- scale(dragons$bodyLength)

#Let's look at the histogram again. The scale has changed, so the distribution is now centered around zero.
hist(dragons$bodyLength_s)  # seems close to normal distribution - good!
```

Why do we standardize/mean-center variables? 
Should we also standardize testScore? Why or why not?


<br><br><br><br><br>


## Linear Regression
<br>
Okay, let's start fitting some lines!
Key Question: Does body length predict test score? 

One way to analyse this data would be to try fitting a linear model to all our data, ignoring all the other variables for now.

This is a "complete pooling" approach, where we "pool" together all the data and ignore the fact that some observations came from specific mountain ranges.

```{r LM}
model <- lm(testScore ~ bodyLength_s, data = dragons)
summary(model)
```

Incredible! It's super significant! We're gonna publish in Nature Dragonology!

<br><br><br>

Let's plot the data with ggplot2 to see the correlation.

```{r LM_Plot}
ggplot(dragons, aes(x = bodyLength, y = testScore)) +
  geom_point()+
  geom_smooth(method = "lm") +
  xlab("Body Length")+
  ylab("Test Score")
```

<br><br><br>

Wait, but we need to check assumptions!

```{r Assumptions}
#Let's plot the residuals from this model. Ideally, the red line should be flat.
plot(model, which = 1)  # not perfect, but looks alright

#Have a quick look at the  qqplot too - point should ideally fall onto the diagonal dashed line
plot(model, which = 2)  # a bit off at the extremes, but that's often the case; again doesn't look too bad
```

<br><br><br>

But linear models also assume that observations are INDEPENDENT. Uh oh.

We collected multiple samples from eight mountain ranges. It's perfectly plausible that the data from within each mountain range are more similar to each other than the data from different mountain ranges - they are correlated. This could be a problem.

```{r Boxplot}
#Have a look at the data to see if above is true
boxplot(testScore ~ mountainRange, data = dragons)  # certainly looks like something is going on here

#We could also plot it colouring points by mountain range
ggplot(dragons, aes(x = bodyLength, y = testScore, colour = mountainRange))+
  geom_point(size = 2)+
  theme_classic()+
  theme(legend.position = "none")
```

Clearly, there is structured variance in our data that has something to do with the mountain range where we found the dragons.

<br><br><br>

How do we deal with this variance? We could run many separate analyses and fit a regression for each of the mountain ranges.
Let's check what it would look like if we fit a separate regression line for each mountain range.

```{r Mountains}
#Lets have a quick look at the data split by mountain range
#We use the facet_wrap to do that
ggplot(data = dragons, aes(x = bodyLength_s, y = testScore)) +
  stat_smooth(method = "lm", se = FALSE, size = 1.5) +
  geom_point() +
  facet_wrap(.~mountainRange) +
  xlab("length") + ylab("test score")
```

From the above plots it looks like our mountain ranges vary both in the dragon body length and in their test scores. This confirms that our observations from within each of the ranges aren't independent. We can't ignore that.

<br><br><br>

So, what if we estimate the effect of bodyLength on testScore for each range independently?

This would be a no-pooling approach: fitting a separate line for each mountain range, and ignoring our group-level information.
This approach treats each group of observations (in this case, mountainRange, but could be participants in other datasets) totally independently.

```{r No_Pooling}
df_no_pooling <- lmList(testScore ~ bodyLength_s | mountainRange, data = dragons) %>% 
  coef() %>% 
  # Mountain Range IDs are stored as row-names above. Let's also add a column to label them.
  rownames_to_column("mountainRange") %>% 
  rename(Intercept = `(Intercept)`, Slope_length = bodyLength_s) %>% 
  add_column(Model = "No pooling")

head(df_no_pooling)
```

Check out the variation in the intercepts and slopes when we fit a separate model for each mountain range.

<br><br><br>

How do our estimates compare for the complete pooling vs. no pooling methods?

```{r No_vs_Complete}
#First, let's grab the coefficients from the first model we fit (the simple linear regression that ignores mountain range information)
df_pooled <- tibble(
  Model = "Complete pooling",
  mountainRange = unique(dragons$mountainRange),
  Intercept = coef(model)[1], 
  Slope_length = coef(model)[2])

#You can see that this just copies the same group-level line estimate for every mountain range
head(df_pooled)

#Let's combine this with the estimates from the no-pooling approach
df_models <- bind_rows(df_pooled, df_no_pooling) %>%
  left_join(dragons, by = "mountainRange")
head(df_models)

#Let's plot the two linear estimates for each mountain range
ggplot(data = df_models, aes (x = bodyLength_s, y = testScore)) + 
  geom_point() +
  geom_abline(aes(intercept = Intercept, slope = Slope_length, color = Model), size = 1)+
  facet_wrap(.~mountainRange)
```

We got very different estimates from the two different approaches. Is there a happy medium?



<br><br><br><br><br>

## Linear Mixed Effects Regression
<br>
Mountain range clearly introduces a structured source of variance in our data. We need to control for that variation if we want to understand whether body length really predicts test scores.

But it doesn't really make sense to estimate the effect for each mountain range separately. Shouldn't we use the full power of our whole dataset?

Mixed effects regression is a compromise: Partial pooling!
We can let each mountain range have it's own regression line, but make an informed guess about that line based on the group-level estimates
This is especially useful when some groups/participants have incomplete data.
<br><br>

Should Mountain Range be a FIXED or RANDOM effect?

We could consider it a FIXED EFFECT if we were interested in testing the hypothesis that location of residence influences test scores-- but that's not our research question! This is just annoying noise that limits our ability to test the relationship between body length and test scores.
So, we need to account for structured variance among mountain ranges by modelling it as a RANDOM EFFECT.

```{r LMER, warning=FALSE, message=FALSE}
#let's fit our first mixed model!
mixed_model <- lmer(testScore ~ bodyLength_s + (1+bodyLength_s|mountainRange), data = dragons)

#note that you might get a "singular fit" warning --> In this case, it's because the data are made-up and we've got some weird correlations in our random effects structure. If you get this with real data, it's not necessarily terrible, but it indicates that you should check for either very high or very low covariance, and try modifying your random effects structure.

#what's the verdict?
summary(mixed_model)
```

<br><br><br>

Now, let's see how these lines compare to the no-pooling method.

```{r Partial_Pooling}
#save the coefficients from our mixed model
df_partial_pooling <- coef(mixed_model)[["mountainRange"]] %>% 
  rownames_to_column("mountainRange") %>% 
  as_tibble() %>% 
  rename(Intercept = `(Intercept)`, Slope_length = bodyLength_s) %>% 
  add_column(Model = "Partial pooling")

#take a peek
head(df_partial_pooling)

#add these estimates to our dataframe
df_models <- bind_rows(df_pooled, df_no_pooling, df_partial_pooling) %>% 
  left_join(dragons, by = "mountainRange")

#Let's plot the three linear estimates for each mountain range
ggplot(data = df_models, aes (x = bodyLength_s, y = testScore)) + 
  geom_point() +
  geom_abline(aes(intercept = Intercept, slope = Slope_length, color = Model), size = 1)+
  facet_wrap(.~mountainRange)
```

You can see that we get different estimates from all three approaches. Partial pooling yields lines that are tailored to each participant, but still influenced by the pooled group data.

<br><br><br>

Overall, it looks like when we account for the effect of mountain range, there is no relationship between body length and test scores.
Well, so much for our Nature Dragonology paper!

Unless... What about our other variables? Let's test whether diet is related to test scores instead.

```{r Diet, warning=FALSE, message=FALSE}
mixed_model <- lmer(testScore ~ diet + (1+diet|mountainRange), data = dragons)

#view output
Anova(mixed_model, type=3)
summary(mixed_model)
```

What did we find?

<br><br><br>

Let's visualize the effect of diet on test scores.

```{r Diet_Plots}
#Plot average test score by diet type
ggplot(dragons, aes(x = diet, y = testScore)) +
  geom_bar(stat = "summary")+
  xlab("Body Length")+
  ylab("Test Score")

#Let's also look at the effect across mountain ranges.
ggplot(dragons, aes(x = diet, y = testScore)) +
  geom_bar(stat = "summary")+
  xlab("Body Length")+
  ylab("Test Score") +
  facet_wrap(.~mountainRange)
```

Looks pretty consistent, but there's obviously still variability between mountains.

<br><br><br>

Let's also plot the predicted values from our mixed effects model, so we can control for the effect of mountain range.

```{r Diet_Pred}
plot_model(mixed_model, type = "pred", terms = "diet")
```

<br><br><br>

Awesome, we will want to put this finding in our Nature Dragonology paper! Let's clean up the output.

```{r Diet_Table}
#Generate an HTML table with parameter estimates
tab_model(mixed_model, p.val = "satterthwaite", show.df = TRUE, p.style="numeric_stars", show.fstat = TRUE, output='html')
```

<br><br><br>

What if we want to test multiple variables at the same time?

```{r Multiple Regression}
mixed_model <- lmer(testScore ~ diet * bodyLength_s + (1+diet*bodyLength_s|mountainRange), data = dragons)
Anova(mixed_model, type=3)
summary(mixed_model)
```

Looks like the effect of diet is still significant after we control for body length. We can also see that there's no significant interaction between diet and body length.

<br><br><br>

Let's plot the output again.
We can also add multiple terms to our plot of model-predicted values. This is especially helpful when you DO have a significant interaction, and you need to understand why!

```{r Multiple_Pred}
plot_model(mixed_model, type = "pred", terms = c("diet", "bodyLength_s"))
#What are the -1, 0, and +1 levels?

#What happens if we swap the order of the terms?
plot_model(mixed_model, type = "pred", terms = c("bodyLength_s", "diet"))
```

Which version of the plot makes more sense to you?

<br><br><br>

Your turn! Try modifying the model above to test whether color is related to testScore, and whether color interacts with diet or bodyLength.

```{r LMER Practice}
#Build your model here


#View the output of the model here



#Plot your results below:


```


<br><br><br><br><br>


## Mixed Effects Logistic Regression
<br>

Okay, let's test a new question.
Test scores are lame. I actually want to know about which dragons breathe fire. This has way more important practical implications, and is more likely to get me grant funding. 

Good news: we have data on fire breathing!
Bad news: It's a binary variable, so we need to change our model.

```{r GLMER, warning=FALSE, message=FALSE}
logit_model <- glmer(breathesFire ~ color + (1+color|mountainRange), data = dragons, family = binomial)
summary(logit_model)
```

<br><br><br>

Let's plot the proportion of dragons that breathe fire by color.

```{r Fire_Plot}
ggplot(dragons, aes(x = color, y = breathesFire)) +
  geom_bar(stat = "summary")+
  xlab("Color") +
  ylab("Proportion that Breathes Fire")
```

<br><br><br>

Let's also get the predicted values from our mixed effects model, so we can control for the effect of mountain range.

```{r Fire_Pred}
plot_model(logit_model, type = "pred", terms = "color")

#generate an HTML table with parameter estimates
tab_model(logit_model, show.df = TRUE, p.style="numeric_stars", show.fstat = TRUE)
```

<br><br><br>

Your turn! Test whether other variables predict breathesFire.

```{r GLMER Practice}
#Build your model here:



#Plot your results:

```



<br><br><br><br><br>

## Convergence
<br>
Some parting tips and tricks:

If you ever encounter an error message about "convergence failure," you cannot trust the results of your model! What can you do to fix this error?

First, check that your continuous predictor variables are all scaled/mean-centered.

Make sure your random effects structure is correct. Are you accidentally specifying random effects that don't make sense for the data? For example, you can never use a between-subs variable as a random slope when you have a random intercept term for subjects. That wouldn't make sense, because you only have one observation per subject. 

You can also try simplifying your random effects structure. You can start with a maximal model (all possible random slopes and intercepts), and then incrementally prune away random effects terms (starting with interactions in random slopes, etc.) until you achieve convergence. You can also formally compare model fits to determine the optimal random effects structure that is justified by the data.

Lastly, you may need to change the settings of your model and increase the maximum number of iterations. Adding the following code within your lmer() call may help: control=lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=10e4))

Check that your random effects variables have at least 5 levels each (e.g., 5+ subjects, 5+ observation sites, etc.)

If these tricks still doesn't help, it may be that you just don't have enough data to fit the model appropriately! You must have enough observations for every combination of fixed and random effect in order to estimate the variance. You may need to get more data, or prune your model to remove interaction terms that may be causing the problem. For example, a 2x2 interaction term between factor variables assumes that you have data for all 4 possible combinations. If you add more and more interactions, you're creating a lot of cells that you need to fill.
