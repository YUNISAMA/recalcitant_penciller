---
title: "Interpreting Regression Coefficients"
author: kevin
categories: [ tutorial ]
image: assets/images/2021-07-05-contrasts/RawData-1.png
featured: true
hidden: false
output:
  html_document: default
  pdf_document: default
  md_document:
    variant: gfm
---

```{r setup, include=FALSE, cache=FALSE}
knitr::opts_knit$set(base.dir="../", base.url="/")
knitr::opts_chunk$set(fig.path="assets/images/2021-07-05-contrasts/",
                      fig.align = 'center',
                      cache.path = '../cache/',
                      message=FALSE, warning=FALSE,
                      cache=TRUE, echo=TRUE, results='hold')
```

Have you ever ran a regression and wondered where the coefficients
come from or what they mean? Or perhaps you've tried the same analysis
with different coding schemes, and the effect was significant one way
but not the other way and you just couldn't put together why. Maybe
you're new to regression and you just want to know what that's all
about. Or maybe you've recently came across my [intro to Bayesian
regression tutorial](https://dukeneuromethods.github.io/brms-intro/)
and you're trying to figure out how to specify priors for regression
coefficients. In any case, this tutorial is for you!

## Table of Contents
- [Setup](#setup)
- [The base case](#the-base-case)
- [Simple linear regression](#simple-linear-regression)
  - [Dummy coding](#dummy-coding)
  - [Sum-to-zero coding](#sum-to-zero-coding)
  - [Thinking like a linear model](#thinking-like-a-linear-model)
- [Regression with additive predictors](#regression-with-additive-predictors)
- [Regression with interacting predictors](#regression-with-interacting-predictors)
- [Continuous variables](#continuous-variables)
  - [To $\beta$ or not to $\beta$?](#to-beta-or-not-to-beta)
  - [Combining continuous & categorical](#combining-continuous-and-categorical)
- [Link functions and you](#link-functions-and-you)
- [Wrapping up](#wrapping-up)

## Setup

First, we'll just get everything set up. We need to tweak some
settings, load packages, and simulate some data. This time around,
let's simulate what some data might look like for a version of the
classic Stroop task. If you're not familiar with the Stroop task, the
idea is that you want to say the *color* of some word on the screen,
regardless of what the text actually says. Typically, people find it
easier to do this when the text and the colors are the same (e.g.,
"blue" in blue text) compared to when the text and the colors are
different (e.g., "red" in blue text). So, people are usually faster to
respond, as well as less likely to make mistakes, when the color and
text match. In this version of our task, let's imagine that we also
manipulated the saturation of the colors to see if that has an effect
as well.

To start, let's simulate some data. Below I use the `expand_grid`
function to generate a number of trials in the two conditions for a
set of fake participants. Next I use `group_by` and `mutate` to
randomly generate a participant-level mean and effect size. Finally, I
randomly sample a reaction time and a response for each trial centered
around the participant-level means, and sort the dataframe. This setup
(where the trial-level data are centered around participant-level
means, which are centered around the group-level mean) reproduces the
kind of data that you would typically obtain experimentally, since
participants are likely to be somewhat different from each other. I
made up the means and effect sizes for this example, but in practice
you will want to look at previous research to find numbers that make
sense. Though ideally we would use [mixed-effect
regression](https://dukeneuromethods.github.io/lmer-intro/) for this
type of data, today we're going to keep things simple with good old
OLS regression.

```{r results='hide'}
## change some settings
## options(contrasts = c("contr.sum","contr.poly")) 
## this tweaks makes sure that contrasts are interpretable as main effects

# load packages
library(tidyverse)   # for data wrangling
library(emmeans)     # for estimating means

## simulate some fake data for a stroop task
set.seed(2021)   ## make sure we get the same data every time
colors <- c('red', 'blue', 'green')
df <- expand_grid(condition=c('same', 'different'),
                  trial=1:25,
                  participant=1:25) %>%
    group_by(participant) %>%
    mutate(color=sample(colors, n(), replace=TRUE),
           saturation=runif(n()),
           participant=as.factor(participant),
           mean=rnorm(1, 3, 0.4)-saturation*rnorm(1, 0.5, 0.2),
           effect=.3+saturation*rnorm(1, 0.25, 0.2) - .1*mean) %>%
    rowwise() %>%
    mutate(text=ifelse(condition=='same', color, sample(colors[!colors %in% color], 1, replace=TRUE))) %>%
    ungroup() %>%
    mutate(RT=rnorm(n(), ifelse(condition=='same', mean, mean+effect), 0.4),
           correct=rbinom(n(), size=1,
                          plogis(ifelse(condition=='same',
                                        mean-2.8+3*saturation*plogis(effect),
                                        mean-2.8+saturation*plogis(effect)))),
           response=ifelse(correct, color, text),
           saturation=saturation*100) %>%
    select(-mean, -effect) %>%
    relocate(participant, trial, condition, saturation, color, text, RT, response, correct) %>%
    arrange(participant, trial, condition)
```


 Let's take a peek at this data and see what we've made:

```{r}
df
```

As we can see, we now have a nice & tidy dataframe where each row is a
trial for a particular participant/condition, level of saturation. As
dependent variables, we have the reaction time, the chosen response,
and whether or not the response was correct. 


## The base case
To kick things off, let's do the simplest model we can make: the null
model. This model predicts reaction time with only a constant
intercept (denoted by the `1` in our formula):

```{r lm-null}
m <- lm(RT ~ 1, df)
summary(m)
```

You can see that our model indeed has only one parameter, an
intercept. What does it correspond to? Well, given no other
information, the model's best guess of a given RT is simply the mean
RT. We can test the mean RT against 0 using a one-sample t-test:

```{r lm-null-mean}
t.test(df$RT)
```

Unsurprisingly, the one-sample t-test gives us exactly the same
results. So, our null model is testing whether the mean RT is
different from zero (and it is!). This isn't especially helful in our
case, but this is all that's needed for many research questions.

## Simple linear regression
To do something a little more useful, let's look to see if our fake
participants were indeed slower when the color of the text was
different from the text itself:

```{r RawData}
ggplot(df, aes(x=condition, y=RT)) +
    geom_violin() +
    geom_jitter(aes(color=participant), height=0, show.legend=F, alpha=0.2) +
    stat_summary(fun=mean, size=1) +
    xlab('Condition') + ylab('Reaction Time (s)') +
    theme_classic(base_size=24)
```

The means are pretty close to each other, but it looks like we might
have the effect we observed. To find out for sure, let's run a regression!

```{r lm-condition}
m <- lm(RT ~ condition, df)
summary(m)
```

Based on the number of stars in this output, it looks like we have a
significant effect! But *what* exactly is significant? More
specifically, what do these coefficients actually mean? The answer
actually depends on how the contrasts over your variables are
coded.

### Dummy coding
By default, `R` uses a system of contrasts called *dummy coding* (or
`contr.treatment`). With dummy coding, we select one of the levels of
the variable as a reference level. For categorical variables, by
default, `R` assumes that you want use the earliest level of the
variable as the reference level (in our case, this is the 'different'
condition). Then the `Intercept` is simply the mean RT for this
reference level. We can confirm this by calculating the mean manually:

```{r dummy-intercept}
mean.different <- df %>% filter(condition=='different') %>% pull(RT) %>% mean
mean.different
```

Then, the coefficient `conditionsame` is the difference between the
means for our two conditions:

```{r dummy-slope}
mean.same <- df %>% filter(condition=='same') %>% pull(RT) %>% mean
mean.same - mean.different
```

Putting this together, our significant result is that reaction times
were slightly shorter for the "same" condition compared to the
"different" condition. What if we want the other way around? Easy- we
just set the desired reference level of the `condition` variable:

```{r lm-condition-rev}
df <- df %>% mutate(condition=factor(condition, levels=c('same', 'different')))
m <- lm(RT ~ condition, df)
summary(m)
```

We can see that the `Intercept` now refers to the average RT in the
"same" condition, and the `conditiondifferent` coefficient now refers
to the difference between the same and different conditions. Notably,
this is the same value as before, but negative.

### Sum-to-zero coding
Another popular way of coding regression coefficients is to use
*sum-to-zero* or *effects* coding (or `contr.sum`), which is popular
in psychology because it produces orthogonal (read: independent)
contrasts similar to an ANOVA. There are a couple different ways to
change the contrast coding in `R`, but I recommend doing it like so:

```{r lm-condition-sum}
m <- lm(RT ~ condition, df, contrasts=list(condition=contr.sum))
summary(m)
```

Under sum-to-zero coding, the intercept is now the grand mean RT over
all of our data (more specifically, the mean of the mean RTs in each
condition), and the slope `condition1` is the difference between this
grand mean and the mean RT of the "same" condition.

```{r sum-intercept-slope}
mean.grand <- mean(c(mean.same, mean.different))
mean.grand

mean.same - mean.grand
```

### Thinking like a linear model
From the last two examples, hopefully it's obvious that regression
coefficients are just differences between means. Specifically, our
regression model sees the world through a table like this:

<style type="text/css">
.tg  {border-collapse:collapse;border-color:#ccc;border-spacing:0;margin:0px auto;}
.tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:1px;color:#333;
  overflow:hidden;padding:10px 20px;word-break:normal;}
.tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:1px;color:#333;
  overflow:hidden;padding:10px 20px;word-break:normal;}
.tg .tg-di1h{border-color:#656565;text-align:center;vertical-align:middle}
.tg .tg-o36t{background-color:#c0c0c0;border-color:#656565;text-align:center;vertical-align:middle}
</style>
<table class="tg">
<tbody>
  <tr>
    <td class="tg-o36t" colspan="2">condition</td>
  </tr>
  <tr>
    <td class="tg-di1h">same</td>
    <td class="tg-di1h">different</td>
  </tr>
</tbody>
</table>
<br>

That is, for each cell in this table, our model predicts a unique mean
reaction time. In this case, the predicted means are just the raw
means that we calculated above. But since this won't work for every
design, I'm going to introduce you to the treasure that is
`emmeans`. `emmeans` stands for "expected marginal means" and does
exactly that: it takes a regression model and a set of independent
variables, and tells you what the model thinks the mean is for setting
of the independent variables. For example, let's get the expected
marginal means of RT by condition:

```{r emmeans-intro}
emmeans(m, ~ condition)
```

To reproduce the model's coefficients, we can just pipe the output of
this statement into the contrast function. I don't know why, but
`emmeans` names the contrasts differently from base `R` (helpful,
right?), such that `contr.treatment` is called `trt.vs.ctrl`, and
`contr.sum` is called `eff`:

```{r emmeans-contrasts}
## for dummy contrasts
emmeans(m, ~ condition) %>%
    contrast('trt.vs.ctrl')

## for sum-to-zero contrasts
emmeans(m, ~ condition) %>%
    contrast('eff')
```

Alternatively, we can put the contrasts directly into the left side of
the formula in the `emmeans` call to see the contrasts alongside the
means:

```{r emmeans-contrasts2}
## for dummy contrasts
emmeans(m, trt.vs.ctrl ~ condition)

## for sum-to-zero contrasts
emmeans(m, eff ~ condition)
```

I'll also note here that this is a fantastic way to get predicted
means/CIs to make a plot of your results, without having to calculate
anything manually (and yes, this even works for mixed-effect models
and repeated measures designs)! Let's try it out:

```{r emmeans-plot}
ggplot(df, aes(x=condition, y=RT)) +
    geom_jitter(aes(color=participant), height=0, show.legend=F, alpha=0.2) +
    geom_pointrange(aes(y=emmean, ymin=lower.CL, ymax=upper.CL),
                    data=as.data.frame(emmeans(m, ~ condition))) +
    xlab('Condition') + ylab('Reaction Time (s)') +
    theme_classic(base_size=24)
```


## Regression with additive predictors
Until now, we've focused on interpreting regression coefficients when
we have just one predictor in our model. But things get a little
messier when we use two or more predictors. For example, perhaps we
wanted to know if the color of the stimulus also effects people's
reaction time in the Stroop task. More specifically, we'd like to
estimate a mean RT for each cell of the following table:

<style type="text/css">
.tg  {border-collapse:collapse;border-color:#ccc;border-spacing:0;margin:0px auto;}
.tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:1px;color:#333;
  overflow:hidden;padding:10px 20px;word-break:normal;}
.tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:1px;color:#333;
  font-weight:normal;overflow:hidden;padding:10px 20px;word-break:normal;}
.tg .tg-di1h{border-color:#656565;text-align:center;vertical-align:middle}
.tg .tg-janv{background-color:#00000000;border-color:#00000000;text-align:center;vertical-align:middle}
.tg .tg-o36t{background-color:#c0c0c0;border-color:#656565;text-align:center;vertical-align:middle}
</style>
<table class="tg">
<tbody>
  <tr>
    <td class="tg-janv"></td>
    <td class="tg-o36t" colspan="2">condition</td>
  </tr>
  <tr>
    <td class="tg-o36t" rowspan="3">color</td>
    <td class="tg-di1h">same, blue</td>
    <td class="tg-di1h">different, blue</td>
  </tr>
  <tr>
    <td class="tg-di1h">same, green</td>
    <td class="tg-di1h"><span style="font-weight:normal;font-style:normal;text-decoration:none">different, green</span></td>
  </tr>
  <tr>
    <td class="tg-di1h">same, red</td>
    <td class="tg-di1h">different, red</td>
  </tr>
</tbody>
</table>
<br>

There are two ways we could add this into our model. In the simplest
case, we might want to see if both (a) condition and (b) color
independently. That is, we force the effect of `condition` to be the
same across the different levels of `color` and vice versa. We can do
this using the `+` symbol in our regression formula:

```{r lm-additive-dummy}
m <- lm(RT ~ condition + color, df)
summary(m)
```

It looks like we have a significant effect of condition, and no
significant effects of color. To replicate these coefficients, we can
use `emmeans` (the `adjust='none'` option is used to make sure the
*p*-values match what we get from `lm`):

```{r emmeans-additive-dummy}
emmeans(m, ~ condition) %>% contrast('trt.vs.ctrl')
emmeans(m, ~ color) %>% contrast('trt.vs.ctrl', adjust='none')
```

We can see that the `Intercept` is our estimate of the mean RT for
trials in the "same" condition with a blue stimulus, the
`conditiondifferent` coefficient is the average increase in RT for the
different condition over the three colors, and the two `color`
coefficients are the difference between mean RT in the red/green and
blue colors, averaging over the two conditions. To confirm that our
model actually treats the effects of condition and color
independently, we can compute contrasts of one variable over the
levels of the other variable:

```{r emmeans-additive-dummy2}
emmeans(m, ~condition|color) %>% contrast('trt.vs.ctrl')
emmeans(m, ~color|condition) %>% contrast('trt.vs.ctrl', adjust='none')
```

Since the contrasts look the same over each level of the variable, we
can be sure that our model does not allow condition and color to
interact. If we want to use different contrast coding for this model,
the coefficients are just as interpretable. For example, we can
estimate the same model with sum-to-zero coefficients:

```{r lm-additive-sum}
m <- lm(RT ~ condition + color, df, contrasts=list(condition='contr.sum', color='contr.sum'))
summary(m)
```

The coefficients can be interpreted in the same way as sum-to-zero
contrasts with only one variable: the intercept corresponds to our
estimate of the grand mean, the coefficient for `condition` is the
difference between the grand mean and the same condition, and the
color coefficients are the difference between the grand mean and the
red/green colors:

```{r emmeans-additive-sum}
emmeans(m, ~ condition) %>% contrast('eff')
emmeans(m, ~ color) %>% contrast('eff', adjust='none')
```

Of course, we can always mix + match the two coding systems together if we so choose:
```{r lm-additive-mix}
m <- lm(RT ~ condition + color, df, contrasts=list(color='contr.sum'))
summary(m)
```
 
 As an exercise, see if you can figure out where these numbers come
 from (hint: look up!).
 

## Regression with interacting predictors
While the additive model is nice and easy to interpret, it's not
always the case that the effect of one variable will be the same
across the domain of another variable. To relax this assumption, we
can fit a multiplicative model that includes interaction terms:

```{r lm-mult-dummy}
m <- lm(RT ~ condition * color, df)
summary(m)
```

You can see that now we have two more interaction coefficients. Under
dummy coding, the presence of these interaction coefficients changes
how the above coefficients are interpreted: now, each coefficient is
conditional with respect to the reference level of each variable. The
`Intercept` is still the mean RT for trials in the "same" `condition`
with a blue stimulus. But, whereas the `conditiondifferent` used to
tell us the average difference between same/different trials over the
three colors, now it only tells us that difference for the `blue`
trials. We call this a "simple effect" since its interpretation is
more simple than that of a "main effect" which we'll talk about
soon. In `emmeans`, we can get this contrast for the blue trials only
using the `at` argument:

```{r emmeans-mult-dummy}
emmeans(m, ~ condition | color, at=list(color=c('blue'))) %>%
    contrast('trt.vs.ctrl')
```

The `colorgreen` and `colorred` coefficients have also changed: now
they are the difference between the blue and red/green colors within
the "same" condition:

```{r emmeans-mult-dummy2}
emmeans(m, ~ color | condition, at=list(condition=c('same'))) %>%
    contrast('trt.vs.ctrl', adjust='none')
```

Finally, we have the two interaction terms. There are two equally good
ways of thinking about them. On one hand, they are the difference in
the same/different Stroop effect between the red/green and blue
colored trials:

```{r emmeans-mult-dummy3}
emmeans(m, ~ condition | color) %>%
    contrast('trt.vs.ctrl') %>%
    contrast(method='trt.vs.ctrl', by='contrast', adjust='none')
```

This tells us that the Stroop effect was larger for green than for
blue stimuli, and larger for blue than for red stimuli (though none of
these differences were significant). But another way of looking at it
is that they describe the difference in the differences between
stimulus colors across the same/different conditions:

```{r emmeans-mult-dummy4}
emmeans(m, ~ color | condition) %>%
    contrast('trt.vs.ctrl') %>%
    contrast(method='trt.vs.ctrl', by='contrast', adjust='none')
```

This perspective tells us that the difference between RTs following
green and blue stimuli in the "different" condition is larger than in
the "same" condition, and that the difference between RTs following
red and blue stimuli in the "different" condition is smaller than in
the "same" condition. In this case the first interpretation is more
natural, but mathematically both interpretations are equivalent, so
feel free to take whichever interpretation seems more natural.

If you're used to running ANOVAs, then it might seem that the approach
of looking at effects of one variable conditional on another is more
complicated than it needs to be. What you *really* want to know is, on
average, what does effect does each predictor have on the outcome
variable- you want a main effect. We can get main effects by switching
from dummy contrasts to sum-to-zero contrasts:

```{r lm-mult-sum}
m <- lm(RT ~ condition * color, df, contrasts=list(condition='contr.sum', color='contr.sum'))
summary(m)
```

 Now we can go back to treating each coefficient in isolation: the
`Intercept` is the grand mean, the `condition1` coefficient tells us
the average difference between the same condition and the grand mean
over all stimulus colors, the `color1` and `color2` are the average
differences between the blue/green colors and the grand mean over both
conditions, and finally the interaction terms are the difference in
the difference in the same condition and the grand mean between the
blue/green colors and the grand mean. Even though this sounds like a
mouthful, sum-to-zero contrasts have the advantage that each
coefficient is independent of the others.


## Continuous variables
We've been through interpreting the null regression model,
interpreting single-variable regression models, and interpreting
regression models with more than one variable. What could possibly be
left? Well, if you remember back to when we simulated this data, we
also simulated the saturation level of the stimulus color (ranging
from 0 to 100%). This is a continuous variable, which has some quirks of
its own in terms of interpreting regression coefficients. Let's start
by forgetting about the Stroop effect, and just looking at whether
saturation influences reaction time during the Stroop task:

```{r saturation-plot}
ggplot(df, aes(x=saturation, y=RT)) +
    geom_point(alpha=.1) +
    geom_smooth(color='black', method='lm') +
    xlab('Saturation (%)') + ylab('Reaction Time (s)') +
    theme_classic(base_size=24)
```

<a name="to-beta-or-not-to-beta"></a>

### To $\beta$ or not to $\beta$?
It turns out that there are a few different ways to code this very
same regression. Let's start by running a regression using `R`'s
default settings:

```{r}
m <- lm(RT ~ saturation, df)
summary(m)
```

With this default setting, we interpret the coefficients similarly to
dummy-coded categorical variables. That is, the `Intercept` is the
mean RT when `saturation` is zero, and the `saturation` coefficient is
the mean increase/decrease in RTs when `saturation` is increased
by 1. Since our `saturation` variable is in percentage units, this is
the increase in RT with a 1% increase in saturation. If you want to
make your `Intercept` more interpretable, you can *standardize* the
predictor variable `saturation` by subtracting the mean and dividing
by the standard deviation (done by the `scale` function):

```{r}
df$saturation.std <- scale(df$saturation)
m <- lm(RT ~ saturation.std, df)
summary(m)
```

Though the `Intercept` is still the mean RT when `saturation.std` is
zero, this has a different interpretation before. Since
`saturation.std` is standardized, it is centered at zero and has a
standard deviation of 1. So, the `Intercept` is the mean RT at the
mean value of `saturation`, and the coefficient `saturation.std` is
the mean increase in RT with a standard deviation increase in
`saturation`. In cases where the predictor variable has a meaningless
scale (like a Likert scale), this coefficient can be more
interpretable than the default coefficient. Finally, in addition to
standardizing the predictor variable `saturation`, we can also
standardize the outcome variable `RT`:

```{r}
df$RT.std <- scale(df$RT)
m <- lm(RT.std ~ saturation.std, df)
summary(m)
```

Now, the `Intercept` is extremely close to zero (representing the mean
standardized RT at the mean saturation), and the coefficient
`saturation.std` is the standardi deviation increase in RT given a
standard deviation increase in saturation: in other words, it's a
correlation!

```{r correlation}
cor.test(df$RT.std, df$saturation.std)
```

You can also run the regression with standardized RTs, but not
standardized saturation values. As an exercise, try interpreting these
(standardized) coefficients:

```{r}
m <- lm(RT.std ~ saturation, df)
summary(m)
```

If you have been paying attention, you'll have noticed that the
significance of the `saturation` coefficient was exactly the same
between the four versions of the model. Ultimately, the choice to
standardize or not to standardize is totally up to you and your
preferences.

<a name="combining-continuous-and-categorical"></a>

### Combining continuous & categorical
We found that saturation decreased RTs in the Stroop task on
average. Could this effect differ depending on whether the color of
the stimulus matched the text? First, let's make a plot:

```{r interaction-plot}
ggplot(df, aes(x=saturation, y=RT, color=condition)) +
    geom_point(alpha=.1) +
    geom_smooth(method='lm') +
    xlab('Saturation (%)') + ylab('Reaction Time (s)') +
    theme_classic()
```

It looks like RTs only decrease with saturation when the color is the
same as the text. To confirm, let's run a (multiplicative) regression:

```{r}
m <- lm(RT ~ condition * saturation, df)
summary(m)
```

Just like before, the `Intercept` is the mean `RT` when `condition` is
at its reference level ("same"), and when `saturation` is at zero,
`conditiondifferent` tells us the mean Stroop effect when `saturation`
is at zero, and `saturation` tells us the increase in `RT` with a 1%
increase in `saturation` in the "same" condition:

```{r}
## Intercept
emmeans(m, ~condition, at=list(condition='same', saturation=0))

## conditiondifferent
emmeans(m, ~condition, at=list(saturation=0)) %>%
    contrast('trt.vs.ctrl')

## saturation
emmeans(m, ~saturation, at=list(saturation=0:1, condition='same')) %>%
    contrast('trt.vs.ctrl')
```

As with the interaction between `condition` and `color`, there are two
different ways we can choose to interpret the interaction
coefficient. First, we could say that it is the difference in size of
the Stroop effect between when `saturation` = 1% and when `saturation`
= 0%. But it's equally accurate to say that it represents the
difference in the effect of saturation between the "same" and
"different" conditions:

```{r}
emmeans(m, ~ saturation | condition, at=list(saturation=0:1)) %>%
    contrast('trt.vs.ctrl') %>%
    contrast(method='trt.vs.ctrl', by='contrast', adjust='none')

emmeans(m, ~ condition | saturation, at=list(saturation=0:1)) %>%
    contrast('trt.vs.ctrl') %>%
    contrast(method='trt.vs.ctrl', by='contrast', adjust='none')
```

As an exercise, try repeating this regression with sum-to-zero
contrasts and interpreting those coefficients!


## Link functions and you
There's one more case where you might be having trouble interpreting
regression coefficients: when using a family other than the default
Gaussian, particilarly families with non-identity link functions. In
logistic regression, for example, we predict a binary outcome with a
linear regression on the `logit` scale (from $-\infty$ to $\infty$),
which gets pumped through the inverse logit function to yield
probabilities (from 0 to 1). This transformation can sometimes make
interpreting the coefficients difficult. As an example, let's try
looking at whether, in addition to being slower, people also make more
mistakes when the stimulus text and color are different. First, as
always, let's check out a plot:

```{r correct-plot}
ggplot(df, aes(x=condition, y=correct)) +
    geom_jitter(alpha=.1, height=0) +
    stat_summary(fun.data=mean_cl_normal) +
    xlab('Condition') + ylab('Probability of Correct Response') +
    theme_classic()
```

It appears as if people perform above chance in the "same" condition,
but not the "different" condition. To confirm, let's look at the
underlying model:

```{r}
m <- glm(correct ~ condition, df, family='binomial')
summary(m)
```

We can see that both the `Intercept` and the `conditiondifferent`
coefficients are significant. But what do they mean? Similarly to
before, the `Intercept` is the mean X for the "same" condition, and
`conditiondifferent` is the mean change in X between the "same" and
"different" conditions. To find out what X exactly is, let's consult
`emmeans`:

```{r}
emmeans(m, ~ condition)
```

`emmeans` tells us that our estimates are on the logit scale, not the
response scale. Although we could transform these manually back to
probabilities using the function `plogis`, `emmeans` can do it for you:

```{r}
emmeans(m, ~ condition, type='response')
```

Now we can see that the probability of responding correctly is 65% in
the same condition and 50% in the different condition. Are these two
probabilities different? Let's find out:

```{r}
emmeans(m, ~ condition, type='response') %>% contrast('trt.vs.ctrl')
```

Whoah- what's going on there? Instead of differences in probabilities,
`emmeans` is giving us something called an odds ratio. What's that?
Well an *odds* is the proportion between something happening and not
happening. We can calculate these manually:

```{r}
p.same <- as.data.frame(emmeans(m, ~ condition, type='response'))$prob[1]
p.diff <- as.data.frame(emmeans(m, ~ condition, type='response'))$prob[2]

odds.same <- p.same / (1 - p.same)
odds.diff <- p.diff / (1 - p.diff)

odds.same
odds.diff
```

We can see that in the "same" condition, participants were 1.9 times
more likely to make a correct response compared to an incorrect
response. In the "different" condition, people were about as likely to
be correct as they were incorrrect. The odds ratio, not surprisingly,
is the ratio between these two odds:

```{r}
odds.ratio <- odds.diff / odds.same
odds.ratio
```

So we can piece all this back together to say that the odds of being
correct were about half as large in the different condition than in
the same condition. Why does `emmeans` give us this weird number?
Well, this is just what logistic regression tests for. Specifically,
the logit scale is also known as the *log odds* scale, which means
that our coefficients are on the scale of log odds. To convert our
coefficients to the odds scale, then, we can just exponentiate:

```{r}
exp(coef(m))  ## get the coefficients on the odds scale

## get the odds with CIs with emmeans
emmeans(m, ~condition, tran='log', type='response')
```

If odds ratios aren't very intuitive for you, you are in good
company. [Some have
argued](https://dukeneuromethods.github.io/journal-club/) that it's
not only fine, but it's preferable to use standard linear regression
for estimating differences in probabilities. But if you're looking for
more practice, as a parting exercise, try running this logistic
regression with condition and saturation as predictors! The
coefficients might take some work to interpret, but the basic logic is
exactly the same.

```{r correct-interaction-plot}
ggplot(df, aes(x=saturation, y=correct, color=condition)) +
    geom_point() +
    geom_smooth(method='glm', method.args=list(family='binomial')) +
    theme_classic()

```

## Wrapping up
If you made it this far, congrats! We covered *a lot* in this
workshop, including how to interpret linear regression coefficients
with zero-two categorical variables, with standardized and
unstandardized continuous variables, and with non-trivial link
functions (using the example of logistic regression). In fact, we
covered so much, that there's not much more to know here! With a
little imagination, you can use the same logic to interpret
coefficients for models with 3+ way interactions, models with random
effects, and models with different link functions and distribution
families.
